{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYv49amhJnHP"
      },
      "outputs": [],
      "source": [
        "!unzip colab.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "StGlgw-gJNG2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from torch.nn import functional as F\n",
        "import random\n",
        "import argparse\n",
        "random.seed(0)\n",
        "\n",
        "from utils import dataset\n",
        "from utils import model \n",
        "from utils import trainer \n",
        "from utils import utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "h916Rj1CJNG-"
      },
      "outputs": [],
      "source": [
        "pretrain_corpus_path = \"data/wiki.txt\"\n",
        "finetune_corpus_path = \"data/birth_places_train.tsv\"\n",
        "eval_corpus_path = \"data/birth_dev.tsv\"\n",
        "\n",
        "vanilla_pretrain_params = \"data/vanilla.pretrain.params\"\n",
        "vanilla_finetune_params = \"data/vanilla.finetune.params\"\n",
        "vanilla_outputs_path = \"data/vanilla.nopretrain.test.predictions.txt\"\n",
        "\n",
        "synthesizer_pretrain_params = \"data/synthesizer.pretrain.params\"\n",
        "synthesizer_finetune_params = \"data/synthesizer.finetune.params\"\n",
        "synthesizer_outputs_path = \"data/vanilla.nopretrain.test.predictions.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dt5uwFaNKdV2"
      },
      "source": [
        "# Vanilla model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VP1TdyarJNG_"
      },
      "outputs": [],
      "source": [
        "block_size = 128\n",
        "text = open(pretrain_corpus_path, encoding=\"utf8\").read()\n",
        "pretrain_dataset = dataset.CharCorruptionDataset(text, block_size)\n",
        "\n",
        "mconf = model.GPTConfig(pretrain_dataset.vocab_size, pretrain_dataset.block_size,\n",
        "    n_layer=4, n_head=8, n_embd=256, synthesizer=False)\n",
        "\n",
        "device = torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "#pretrain\n",
        "\n",
        "m = model.GPT(mconf)\n",
        "\n",
        "tconf = trainer.TrainerConfig(max_epochs=650, batch_size=128, learning_rate=6e-3,\n",
        "                    lr_decay=True, warmup_tokens=512*20, final_tokens=200*len(pretrain_dataset)*block_size,\n",
        "                    num_workers=4)\n",
        "\n",
        "t = trainer.Trainer(m, pretrain_dataset, None, tconf)\n",
        "t.train()\n",
        "torch.save(m.state_dict(), vanilla_pretrain_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hbHmV2u_J7yD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe971e30-81a3-4ab0-fcca-748b269b40a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 3323392\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "epoch 1 iter 7: train loss 0.71099. lr 5.999844e-04: 100%|██████████| 8/8 [00:02<00:00,  3.08it/s]\n",
            "epoch 2 iter 7: train loss 0.57014. lr 5.999351e-04: 100%|██████████| 8/8 [00:02<00:00,  3.06it/s]\n",
            "epoch 3 iter 7: train loss 0.49074. lr 5.998521e-04: 100%|██████████| 8/8 [00:02<00:00,  3.09it/s]\n",
            "epoch 4 iter 7: train loss 0.40915. lr 5.997352e-04: 100%|██████████| 8/8 [00:02<00:00,  3.07it/s]\n",
            "epoch 5 iter 7: train loss 0.34579. lr 5.995847e-04: 100%|██████████| 8/8 [00:02<00:00,  3.03it/s]\n",
            "epoch 6 iter 7: train loss 0.27529. lr 5.994004e-04: 100%|██████████| 8/8 [00:02<00:00,  3.01it/s]\n",
            "epoch 7 iter 7: train loss 0.24950. lr 5.991823e-04: 100%|██████████| 8/8 [00:02<00:00,  2.99it/s]\n",
            "epoch 8 iter 7: train loss 0.20744. lr 5.989306e-04: 100%|██████████| 8/8 [00:02<00:00,  2.94it/s]\n",
            "epoch 9 iter 7: train loss 0.18367. lr 5.986453e-04: 100%|██████████| 8/8 [00:02<00:00,  2.96it/s]\n",
            "epoch 10 iter 7: train loss 0.14004. lr 5.983263e-04: 100%|██████████| 8/8 [00:02<00:00,  2.94it/s]\n",
            "500it [00:50,  9.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct: 123.0 out of 500.0: 24.6%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#finetune\n",
        "\n",
        "m = model.GPT(mconf)\n",
        "m.load_state_dict(torch.load(vanilla_pretrain_params))\n",
        "m = m.to(device)\n",
        "\n",
        "fine_text = open(finetune_corpus_path, encoding=\"utf8\").read()\n",
        "train_dataset = dataset.NameDataset(pretrain_dataset, fine_text)\n",
        "\n",
        "tconf = trainer.TrainerConfig(max_epochs=10, batch_size=256, learning_rate=6e-4,\n",
        "            lr_decay=True, warmup_tokens=512*20, final_tokens=200*len(pretrain_dataset)*block_size,\n",
        "            num_workers=4)\n",
        "\n",
        "t = trainer.Trainer(m, train_dataset, None, tconf)\n",
        "t.train()\n",
        "torch.save(m.state_dict(), vanilla_finetune_params)\n",
        "\n",
        "#evaluation\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with open(vanilla_outputs_path, 'w') as fout:\n",
        "    predictions = []\n",
        "    for line in tqdm(open(eval_corpus_path)):\n",
        "        x = line.split('\\t')[0]\n",
        "        x = x + '⁇'\n",
        "        x = torch.tensor([pretrain_dataset.stoi[s] for s in x], dtype=torch.long)[None,...].to(device)\n",
        "        pred = utils.sample(m, x, 32, sample=False)[0]\n",
        "        completion = ''.join([pretrain_dataset.itos[int(i)] for i in pred])\n",
        "        pred = completion.split('⁇')[1]\n",
        "        predictions.append(pred)\n",
        "        fout.write(pred + '\\n')\n",
        "    total, correct = utils.evaluate_places(eval_corpus_path, predictions)\n",
        "if total > 0:\n",
        "    print('Correct: {} out of {}: {}%'.format(correct, total, correct/total*100))\n",
        "else:\n",
        "    print('Predictions written to {}; no targets provided'\n",
        "            .format(vanilla_outputs_path))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3C7v0xhKdV8"
      },
      "source": [
        "# synthesizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOooOYUfJNHC"
      },
      "outputs": [],
      "source": [
        "block_size = 128\n",
        "text = open(pretrain_corpus_path, encoding=\"utf8\").read()\n",
        "pretrain_dataset = dataset.CharCorruptionDataset(text, block_size)\n",
        "\n",
        "mconf = model.GPTConfig(pretrain_dataset.vocab_size, pretrain_dataset.block_size,\n",
        "    n_layer=4, n_head=8, n_embd=256, synthesizer=True)\n",
        "\n",
        "device = torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "#pretrain\n",
        "\n",
        "m = model.GPT(mconf)\n",
        "\n",
        "tconf = trainer.TrainerConfig(max_epochs=650, batch_size=128, learning_rate=6e-3,\n",
        "                    lr_decay=True, warmup_tokens=512*20, final_tokens=200*len(pretrain_dataset)*block_size,\n",
        "                    num_workers=4)\n",
        "\n",
        "t = trainer.Trainer(m, pretrain_dataset, None, tconf)\n",
        "t.train()\n",
        "torch.save(m.state_dict(), vanilla_pretrain_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdEUJeS2iWNP",
        "outputId": "ed4f102c-4269-41a0-8945-8e9f43e507f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 3076988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "epoch 1 iter 7: train loss 0.77845. lr 5.999844e-04: 100%|██████████| 8/8 [00:02<00:00,  3.18it/s]\n",
            "epoch 2 iter 7: train loss 0.64695. lr 5.999351e-04: 100%|██████████| 8/8 [00:02<00:00,  3.19it/s]\n",
            "epoch 3 iter 7: train loss 0.60176. lr 5.998521e-04: 100%|██████████| 8/8 [00:02<00:00,  3.18it/s]\n",
            "epoch 4 iter 7: train loss 0.53597. lr 5.997352e-04: 100%|██████████| 8/8 [00:02<00:00,  3.13it/s]\n",
            "epoch 5 iter 7: train loss 0.49016. lr 5.995847e-04: 100%|██████████| 8/8 [00:02<00:00,  3.08it/s]\n",
            "epoch 6 iter 7: train loss 0.44557. lr 5.994004e-04: 100%|██████████| 8/8 [00:02<00:00,  3.03it/s]\n",
            "epoch 7 iter 7: train loss 0.39984. lr 5.991823e-04: 100%|██████████| 8/8 [00:02<00:00,  3.14it/s]\n",
            "epoch 8 iter 7: train loss 0.35062. lr 5.989306e-04: 100%|██████████| 8/8 [00:02<00:00,  3.15it/s]\n",
            "epoch 9 iter 7: train loss 0.29478. lr 5.986453e-04: 100%|██████████| 8/8 [00:02<00:00,  3.14it/s]\n",
            "epoch 10 iter 7: train loss 0.26134. lr 5.983263e-04: 100%|██████████| 8/8 [00:02<00:00,  3.28it/s]\n",
            "500it [00:52,  9.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct: 55.0 out of 500.0: 11.0%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#finetune\n",
        "\n",
        "m = model.GPT(mconf)\n",
        "m.load_state_dict(torch.load(vanilla_pretrain_params))\n",
        "m = m.to(device)\n",
        "\n",
        "fine_text = open(finetune_corpus_path, encoding=\"utf8\").read()\n",
        "train_dataset = dataset.NameDataset(pretrain_dataset, fine_text)\n",
        "\n",
        "tconf = trainer.TrainerConfig(max_epochs=10, batch_size=256, learning_rate=6e-4,\n",
        "            lr_decay=True, warmup_tokens=512*20, final_tokens=200*len(pretrain_dataset)*block_size,\n",
        "            num_workers=4)\n",
        "\n",
        "t = trainer.Trainer(m, train_dataset, None, tconf)\n",
        "t.train()\n",
        "torch.save(m.state_dict(), vanilla_finetune_params)\n",
        "\n",
        "#evaluation\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with open(vanilla_outputs_path, 'w') as fout:\n",
        "    predictions = []\n",
        "    for line in tqdm(open(eval_corpus_path)):\n",
        "        x = line.split('\\t')[0]\n",
        "        x = x + '⁇'\n",
        "        x = torch.tensor([pretrain_dataset.stoi[s] for s in x], dtype=torch.long)[None,...].to(device)\n",
        "        pred = utils.sample(m, x, 32, sample=False)[0]\n",
        "        completion = ''.join([pretrain_dataset.itos[int(i)] for i in pred])\n",
        "        pred = completion.split('⁇')[1]\n",
        "        predictions.append(pred)\n",
        "        fout.write(pred + '\\n')\n",
        "    total, correct = utils.evaluate_places(eval_corpus_path, predictions)\n",
        "if total > 0:\n",
        "    print('Correct: {} out of {}: {}%'.format(correct, total, correct/total*100))\n",
        "else:\n",
        "    print('Predictions written to {}; no targets provided'\n",
        "            .format(vanilla_outputs_path))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "m-qYjT91p4ys"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "run.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "e605934006ecb0dad0f6e25284386f8048dc68c73cedad0de4787be096007b39"
    },
    "kernelspec": {
      "display_name": "Python 3.7.13 ('cs224n')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}